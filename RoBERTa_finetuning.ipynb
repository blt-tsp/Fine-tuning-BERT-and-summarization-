{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxGS1foCTUGL58cChsr7QS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blt-tsp/Fine-tuning-BERT-and-summarization-/blob/main/RoBERTa_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning RoBERTa and preprocessing datas\n"
      ],
      "metadata": {
        "id": "Zp5us6UzkasP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline for bin to tensor "
      ],
      "metadata": {
        "id": "dC91Wak0kgSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4KlttYkwbvk",
        "outputId": "cbd155aa-ea78-4a4b-8e23-965ea924cc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.0.post2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.22.4)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import struct\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# Define the PCM parameters\n",
        "bit_depth = 16\n",
        "sample_rate = 8000\n",
        "frame_size = 0.01  # 10 milliseconds\n",
        "frame_step = 0.005  # 5 milliseconds\n",
        "\n",
        "# Load the RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Define a function to convert binary data to PCM\n",
        "def binary_to_pcm(binary_data):\n",
        "    # Convert binary data to array of 16-bit integers\n",
        "    int_data = np.frombuffer(binary_data, dtype=np.int16)\n",
        "    # Normalize the data to the range [-1, 1]\n",
        "    float_data = int_data / 32768.0\n",
        "    # Resample the data to the desired sample rate\n",
        "    resampled_data = librosa.resample(float_data, 44100, sample_rate)\n",
        "    # Convert the data to PCM format\n",
        "    pcm_data = (resampled_data * (2 ** (bit_depth - 1) - 1)).astype(np.int16)\n",
        "    return pcm_data\n",
        "\n",
        "# Define a function to convert PCM data to text\n",
        "def pcm_to_text(pcm_data):\n",
        "    # Convert PCM data to binary string\n",
        "    binary_data = struct.pack('h' * len(pcm_data), *pcm_data)\n",
        "    # Encode binary string as ASCII text\n",
        "    text_data = binary_data.encode('ascii', 'ignore')\n",
        "    return text_data\n",
        "\n",
        "# Define a function to tokenize text data\n",
        "def tokenize_text(text_data):\n",
        "    # Tokenize the text data\n",
        "    tokenized_data = tokenizer.encode(text_data, add_special_tokens=True, max_length=512, truncation=True)\n",
        "    return tokenized_data\n",
        "\n",
        "# Define a function to prepare the data for a single binary file\n",
        "def prepare_data_for_file(file_path):\n",
        "    # Load the binary data from file\n",
        "    with open(file_path, 'rb') as f:\n",
        "        binary_data = f.read()\n",
        "\n",
        "    # Convert binary data to PCM\n",
        "    pcm_data = binary_to_pcm(binary_data)\n",
        "\n",
        "    # Convert PCM data to text\n",
        "    text_data = pcm_to_text(pcm_data)\n",
        "\n",
        "    # Tokenize text data\n",
        "    tokenized_data = tokenize_text(text_data)\n",
        "\n",
        "    # Return the tokenized data\n",
        "    return tokenized_data\n",
        "\n",
        "# Define a function to prepare the data for all binary files in a directory\n",
        "def prepare_data_for_directory(directory_path):\n",
        "    # Get the list of binary files in the directory\n",
        "    file_list = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f)) and f.endswith('.bin')]\n",
        "\n",
        "    # Prepare the data for each file in the directory\n",
        "    tokenized_data_list = []\n",
        "    for file_name in file_list:\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        tokenized_data = prepare_data_for_file(file_path)\n",
        "        tokenized_data_list.append(tokenized_data)\n",
        "\n",
        "    # Convert the list of tokenized data to a tensor\n",
        "    tensor_data = torch.tensor(tokenized_data_list)\n",
        "\n",
        "    # Return the tensor data\n",
        "    return tensor_data\n"
      ],
      "metadata": {
        "id": "QWUAgufmkl_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fine tuning on some of our xml files\n"
      ],
      "metadata": {
        "id": "vUi-PPigko_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Load the labeled dataset\n",
        "input_tensors = torch.load('input_tensors.pt')\n",
        "target_outputs = [ET.parse(xml_file).getroot() for xml_file in os.listdir('target_outputs') if xml_file.endswith('.xml')]\n",
        "\n",
        "# Load the pre-trained RoBERTa model and tokenizer\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)  \n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define a function to train the model\n",
        "def train_model(input_tensors, target_outputs, model, tokenizer, optimizer, loss_fn, num_epochs=10):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tokenize the target outputs\n",
        "    tokenized_target_outputs = [torch.tensor(tokenizer.encode(ET.tostring(target_output).decode('utf-8'), add_special_tokens=True)) for target_output in target_outputs]\n",
        "\n",
        "    # Combine the input tensors and target outputs into a Dataset\n",
        "    dataset = torch.utils.data.TensorDataset(input_tensors, torch.stack(tokenized_target_outputs))\n",
        "\n",
        "    # Define the DataLoader\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "    # Train the model for the specified number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop over the batches in the DataLoader\n",
        "        for batch in dataloader:\n",
        "            # Extract the input and target tensors from the batch\n",
        "            input_ids = batch[0]\n",
        "            target_ids = batch[1]\n",
        "\n",
        "            # Zero out the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute the model output for the input tensor\n",
        "            outputs = model(input_ids)\n",
        "\n",
        "            # Compute the loss between the model output and target tensor\n",
        "            loss = loss_fn(outputs.logits, target_ids)\n",
        "\n",
        "            # Backpropagate the loss and update the model weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Print the loss for the current epoch\n",
        "        print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")\n",
        "\n",
        "# Train the model on the labeled dataset\n",
        "train_model(input_tensors, target_outputs, model, tokenizer, optimizer, loss_fn, num_epochs=10)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('fine_tuned_model')\n"
      ],
      "metadata": {
        "id": "JE6kc7cAk2v9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "eb28a83f-365a-4335-989a-9fc2c932fb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-26210822a979>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the labeled dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_tensors.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtarget_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxml_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target_outputs'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxml_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input_tensors.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "distance between 2 trees"
      ],
      "metadata": {
        "id": "BNKHvnmyl2wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tree_distance(tree1, tree2):\n",
        "    \"\"\"\n",
        "    Calculates the distance between two telecom trees.\n",
        "\n",
        "    Args:\n",
        "        tree1 (xml.etree.ElementTree.Element): The root node of the first tree.\n",
        "        tree2 (xml.etree.ElementTree.Element): The root node of the second tree.\n",
        "\n",
        "    Returns:\n",
        "        The distance between the two trees.\n",
        "    \"\"\"\n",
        "    # If the trees are identical, the distance is 0\n",
        "    if ET.tostring(tree1) == ET.tostring(tree2):\n",
        "        return 0\n",
        "\n",
        "    # If the trees have a different number of children, the distance is the absolute difference in the number of children\n",
        "    if len(tree1) != len(tree2):\n",
        "        return abs(len(tree1) - len(tree2))\n",
        "    # If the trees have the same number of children, calculate the distance between each pair of corresponding children\n",
        "    child_distances = [tree_distance(tree1[i], tree2[i]) for i in range(len(tree1))]\n",
        "\n",
        "    # Return the sum of the distances between the corresponding children\n",
        "    return sum(child_distances)\n"
      ],
      "metadata": {
        "id": "Uw10YpVdlZCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This script loads the fine-tuned RoBERTa model and tokenizer, loads the reference telecom trees from a directory containing XML files, prepares an input tensor for prediction, tokenizes the input tensor, makes a prediction with the model, converts the output to a softmax probability distribution, gets the predicted class, loads the predicted telecom protocol from an XML file based on the predicted class, loops through the list of reference telecom trees and calculates the distance between the predicted telecom protocol and each reference telecom tree, and chooses the reference telecom tree with the smallest distance. The script then prints the XML representation of the matched telecom tree.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DV80GQ6HnDEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRK-A-20sdaX",
        "outputId": "647a5493-1e6f-4292-bf9e-fd1b0f99e950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = RobertaForSequenceClassification.from_pretrained('fine_tuned_model')\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Load the reference telecom trees from XML files\n",
        "ref_trees = []\n",
        "for file in os.listdir('reference_trees'):\n",
        "    if file.endswith('.xml'):\n",
        "        ref_trees.append(ET.parse(os.path.join('reference_trees', file)).getroot())\n",
        "\n",
        "# Prepare the input tensor for prediction\n",
        "input_data = b'\\x00\\x01\\x02\\x03'\n",
        "input_tensor = torch.tensor([input_data])\n",
        "\n",
        "# Tokenize the input tensor\n",
        "input_ids = tokenizer.encode(input_tensor[0].tolist(), add_special_tokens=True, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "# Make a prediction with the model\n",
        "output = model(input_ids)[0]\n",
        "\n",
        "# Convert the output tensor to a softmax probability distribution\n",
        "probs = torch.nn.functional.softmax(output, dim=-1)\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "# Load the predicted telecom protocol from an XML file\n",
        "predicted_protocol = ET.parse(os.path.join('predicted_protocols', f'protocol_{predicted_class}.xml')).getroot()\n",
        "\n",
        "# Calculate the distance between the predicted telecom protocol and each reference telecom tree\n",
        "min_distance = float('inf')\n",
        "matched_tree = None\n",
        "for ref_tree in ref_trees:\n",
        "    distance = tree_distance(predicted_protocol, ref_tree)\n",
        "    if distance < min_distance:\n",
        "        min_distance = distance\n",
        "        matched_tree = ref_tree\n",
        "\n",
        "# Print the matched telecom tree\n",
        "print(ET.tostring(matched_tree))\n"
      ],
      "metadata": {
        "id": "5gdKBZURnQbV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "af5a2686-2d58-4da7-f9ca-f8c8705a2bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/fine_tuned_model/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1196\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     )\n\u001b[0;32m-> 1541\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    290\u001b[0m             )\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-645170c4-226e60931c002ddd317e83e4)\n\nRepository Not Found for url: https://huggingface.co/fine_tuned_model/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6efbf16f1603>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the fine-tuned model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fine_tuned_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   2270\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             logger.warning(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    629\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    425\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: fine_tuned_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing some classes to manipulate data and inference on gpu\n"
      ],
      "metadata": {
        "id": "sb4kF5XHpRxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIXFjm_jvvUQ",
        "outputId": "919adab0-3981-4a47-eea6-bd3622171f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def train_test_split(data, test_size, batch_size, device):\n",
        "      train_data, test_data = torch.utils.data.random_split(data, [len(data) - test_size, test_size])\n",
        "      train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "      test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "      return train_loader, test_loader\n",
        "\n",
        "\n",
        "class TelecomDataset(Dataset):        # binary flow dataset\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.data = []\n",
        "        for file in os.listdir(data_dir):\n",
        "            if file.endswith('.bin'):\n",
        "                with open(os.path.join(data_dir, file), 'rb') as f:\n",
        "                    self.data.append(f.read())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "class TelecomTreeDataset(Dataset):    # telecom tree dataset\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.trees = []\n",
        "        for file in os.listdir(data_dir):\n",
        "            if file.endswith('.xml'):\n",
        "                self.trees.append(ET.parse(os.path.join(data_dir, file)).getroot())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trees)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.trees[idx]\n",
        "\n",
        "class TelecomTreeNet(nn.Module):    \n",
        "    def __init__(self, num_classes):\n",
        "        super(TelecomTreeNet, self).__init__()\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "        self.num_classes = num_classes                        #a discuter avec aurélien\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.roberta.config.hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, self.num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids, attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class TelecomTreeMatcher:\n",
        "    def __init__(self, model_dir, tree_dir):\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "        self.ref_trees = []\n",
        "        for file in os.listdir(tree_dir):\n",
        "            if file.endswith('.xml'):\n",
        "                self.ref_trees.append(ET.parse(os.path.join(tree_dir, file)).getroot())\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        input_ids = []\n",
        "        for d in data:\n",
        "            input_tensor = torch.tensor([d])\n",
        "            input_id = self.tokenizer.encode(input_tensor[0].tolist(), add_special_tokens=True, truncation=True,\n",
        "                                             padding=True, max_length=512, return_tensors='pt')\n",
        "            input_ids.append(input_id)\n",
        "        return input_ids\n",
        "\n",
        "    def match(self, data):\n",
        "        input_ids = self.preprocess(data)\n",
        "        predicted_classes = self.predict(input_ids)\n",
        "        predicted_protocols = [ET.parse(os.path.join('predicted_protocols', f'protocol_{c}.xml')).getroot() for c in\n",
        "                               predicted_classes]\n",
        "        distances = [self.tree_distance(p, t) for p, t in zip(predicted_protocols, self.ref_trees)]\n",
        "        min_distance = min(distances)\n",
        "        matched_tree = self.ref_trees[distances.index(min_distance)]\n",
        "        return ET.tostring(matched_tree)\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_distance(t1, t2):\n",
        "        # recursive function to calculate distance between two trees\n",
        "        pass\n",
        "\n",
        "    def train(self, epochs, batch_size, patience=3):\n",
        "        train_loader = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(self.val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        model = TelecomTreeNet()\n",
        "        model = nn.DataParallel(model)  # Distribute model across multiple GPUs\n",
        "        model.to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters())\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        early_stop_count = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = 0\n",
        "            val_loss = 0\n",
        "\n",
        "            # Training loop\n",
        "            model.train()\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                inputs, labels = batch\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation loop\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    inputs, labels = batch\n",
        "                    inputs = inputs.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                early_stop_count = 0\n",
        "            else:\n",
        "                early_stop_count += 1\n",
        "                if early_stop_count == patience:\n",
        "                    print(f'Validation loss did not improve for {patience} epochs. Training stopped.')\n",
        "                    break\n",
        "\n",
        "            print(f'Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}')\n",
        "\n",
        "        self.model = model.module  # Get the underlying model after training\n",
        "\n",
        "    def predict(self, tensor_data):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            inputs = tensor_data.to(self.device)\n",
        "            outputs = self.model(inputs)\n",
        "            return self._tensor_to_tree(outputs)"
      ],
      "metadata": {
        "id": "xRDdjFS1thg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import transformers\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "class SDHProtocolTrainer:\n",
        "    def __init__(self, model, train_dataset, val_dataset, batch_size, learning_rate, num_epochs, patience=3):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.patience = patience\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.optimizer = transformers.AdamW(model.parameters(), lr=learning_rate)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=len(self.train_dataset) // self.batch_size * self.num_epochs\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        best_val_loss = np.inf\n",
        "        epochs_since_improvement = 0\n",
        "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        for epoch in range(1, self.num_epochs + 1):\n",
        "            print(f'Epoch {epoch}/{self.num_epochs}:')\n",
        "            train_loss = self.train_epoch(train_loader)\n",
        "            val_loss, val_acc = self.evaluate(val_loader)\n",
        "            print(f'Training loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}')\n",
        "            self.scheduler.step()\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                epochs_since_improvement = 0\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "                print('Saved new best model')\n",
        "            else:\n",
        "                epochs_since_improvement += 1\n",
        "                if epochs_since_improvement >= self.patience:\n",
        "                    print(f'Validation loss did not improve for {self.patience} epochs. Training stopped.')\n",
        "                    break\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(self.device)\n",
        "            attention_mask = attention_mask.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model(input_ids, attention_mask)\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        return train_loss / len(train_loader)\n",
        "\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                inputs, labels = batch\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                y_true.extend(labels.cpu().numpy().tolist())\n",
        "                y_pred.extend(preds.cpu().numpy().tolist())\n",
        "                \n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        print('Accuracy: {:.4f}'.format(accuracy))\n",
        "        print('Precision: {:.4f}'.format(precision))\n",
        "        print('Recall: {:.4f}'.format(recall))\n",
        "        print('F1 Score: {:.4f}'.format(f1_score))\n"
      ],
      "metadata": {
        "id": "HGUbTaVDzAJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some visualisations\n"
      ],
      "metadata": {
        "id": "2wEBIMNu10do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart of encaspulated protocols"
      ],
      "metadata": {
        "id": "k6WkgsHp17dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "unique_protocols = set(encapsulated_protocols)\n",
        "counts = [encapsulated_protocols.count(proto) for proto in unique_protocols]\n",
        "\n",
        "plt.bar(unique_protocols, counts)\n",
        "plt.title('Distribution of Encapsulated Protocols')\n",
        "plt.xlabel('Encapsulated Protocols')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IyncBb7X164O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributions of frame lengths"
      ],
      "metadata": {
        "id": "FbfjKRy72Ewd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(sdh_frame_lengths, bins=50)\n",
        "plt.title('Distribution of SDH Frame Lengths')\n",
        "plt.xlabel('SDH Frame Lengths')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tL7DQgUY2Lrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Matrix"
      ],
      "metadata": {
        "id": "vjVKGUaa2bIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "corr_matrix = np.corrcoef(features.T)\n",
        "\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TC3vjbhX2V8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA Analysis"
      ],
      "metadata": {
        "id": "LSpBQeea2hqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(features)\n",
        "\n",
        "plt.scatter(pca_features[:,0], pca_features[:,1], c=encapsulated_protocols)\n",
        "plt.title('PCA Scatterplot of Encapsulated Protocols')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1yWdOAB52hFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "AD-0Rj242oX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming 'model' is your trained model and 'X_test' and 'y_test' are your test set\n",
        "y_pred = model.predict(X_test)\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(conf_mat, annot=True, cmap='coolwarm')\n",
        "plt.title('Confusion Matrix of Test Set')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "seOIoGtw2oFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHAP implementation to gain insights from our outputs"
      ],
      "metadata": {
        "id": "B74quenE3hC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO6nj7FJ3276",
        "outputId": "5a56d556-7cc7-427c-fc9b-4ca210804d8d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting shap\n",
            "  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.22.4)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.65.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.41.0 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "class TreeExplainer:\n",
        "    def __init__(self, model, data):\n",
        "        self.model = model\n",
        "        self.data = data\n",
        "        self.explainer = shap.Explainer(model)\n",
        "        self.shap_values = self.explainer(data)\n",
        "    \n",
        "    def visualize(self, idx):\n",
        "        shap.plots.waterfall(self.shap_values[idx])\n",
        "\n",
        "# create an explainer object for the trained model\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "# get a sample of the test data to compute SHAP values for\n",
        "sample = test_data[0:100]\n",
        "\n",
        "# compute the SHAP values for the sample data\n",
        "shap_values = explainer.shap_values(sample)\n",
        "\n",
        "# summarize the SHAP values for the first sample instance\n",
        "shap.summary_plot(shap_values[0], feature_names=['feat1', 'feat2', 'feat3', ...])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "Kq3DkTkK3g5W",
        "outputId": "80b49991-3329-45b9-b44f-e01396604e5d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-700ab31c4c65>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# create an explainer object for the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# get a sample of the test data to compute SHAP values for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}